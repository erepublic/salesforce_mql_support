/* eslint-disable no-console */

const fs = require("node:fs");
const os = require("node:os");
const path = require("node:path");
const { execFileSync } = require("node:child_process");

function run(cmd, args, opts = {}) {
  return execFileSync(cmd, args, {
    encoding: "utf8",
    stdio: ["ignore", "pipe", "pipe"],
    ...opts
  });
}

function runSfJson(args) {
  const out = run("sf", args);
  return JSON.parse(out);
}

function sleep(ms) {
  Atomics.wait(new Int32Array(new SharedArrayBuffer(4)), 0, 0, ms);
}

function chunk(arr, n) {
  const out = [];
  for (let i = 0; i < arr.length; i += n) out.push(arr.slice(i, i + n));
  return out;
}

function parseArgs(argv) {
  const args = {
    targetOrg: "mql-sandbox",
    batchSize: 5,
    sleepSeconds: 20,
    maxBatches: Infinity
  };
  for (let i = 2; i < argv.length; i++) {
    const a = argv[i];
    if (a === "--target-org") args.targetOrg = String(argv[++i] || "").trim();
    else if (a === "--batch-size") args.batchSize = Number(argv[++i]);
    else if (a === "--sleep-seconds") args.sleepSeconds = Number(argv[++i]);
    else if (a === "--max-batches") args.maxBatches = Number(argv[++i]);
  }
  if (!args.targetOrg) throw new Error("invalid --target-org");
  if (!Number.isFinite(args.batchSize) || args.batchSize <= 0)
    throw new Error("invalid --batch-size");
  if (!Number.isFinite(args.sleepSeconds) || args.sleepSeconds < 0)
    throw new Error("invalid --sleep-seconds");
  if (
    args.maxBatches !== Infinity &&
    (!Number.isFinite(args.maxBatches) || args.maxBatches <= 0)
  ) {
    throw new Error("invalid --max-batches");
  }
  return args;
}

function main() {
  const { targetOrg, batchSize, sleepSeconds, maxBatches } = parseArgs(
    process.argv
  );

  const res = runSfJson([
    "data",
    "query",
    "--target-org",
    targetOrg,
    "--query",
    "SELECT Id FROM MQL__c ORDER BY CreatedDate DESC",
    "--json"
  ]);

  const ids = (res?.result?.records || []).map((r) => r.Id).filter(Boolean);
  console.log(`Found ${ids.length} MQL__c records`);

  const batches = chunk(ids, batchSize).slice(0, maxBatches);
  const tmpFile = path.join(os.tmpdir(), `mql_backfill_${Date.now()}.apex`);

  for (let idx = 0; idx < batches.length; idx++) {
    const batch = batches[idx];
    const setLiteral = batch.map((id) => `'${id}'`).join(", ");
    const apex = [
      "// Auto-generated by regenerate_mql_summaries_sandbox_sequential.js",
      `MqlSummarizerCallout.triggerSummarizationForce(new Set<Id>{ ${setLiteral} });`,
      ""
    ].join("\n");

    fs.writeFileSync(tmpFile, apex, "utf8");
    console.log(
      `Enqueuing batch ${idx + 1}/${batches.length} (size=${batch.length})`
    );

    try {
      run("sf", ["apex", "run", "--target-org", targetOrg, "--file", tmpFile], {
        stdio: ["ignore", "pipe", "pipe"]
      });
    } catch (e) {
      console.error("Batch enqueue failed", {
        idx: idx + 1,
        message: e?.message
      });
    }

    if (idx < batches.length - 1 && sleepSeconds > 0) {
      sleep(sleepSeconds * 1000);
    }
  }

  try {
    fs.unlinkSync(tmpFile);
  } catch {
    // ignore
  }

  console.log(
    "Done enqueuing batches. Summaries update asynchronously; rerun the check script in a few minutes."
  );
}

main();
